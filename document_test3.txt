Hello, this is Pratap and I'm the product manager for OCI language. In this video, I'm going to show you a quick demo through OCI console which demonstrates the capabilities of the language service. In language service, we have APIs for analyzing text as well as translating the text. For analyzing the text, we have pre-trained models for detecting the language, extracting the named entities, classifying the text and extracting the key phrases, and doing sentiment analysis, also identifying the private information. Text translation offers you to translate both plain text as well as the HTML text. and custom models provide flexibility to customize the models to detect the entities which is of interest to you and I will show you how to define the entities and the classes and train a custom model in the video. Let me take you to OCI Consult. Once you log into Oracle Cloud, you can access the language service by going to analytics and AI and selecting language service. To perform analysis using pre-trained models you can select text analytics and if you click on analyze language service models will show you the pre-trained language detection shows this text is in English. If I replace the text with this text which is in French I expect the model to detect this as a French. Similarly, the classification model that the topic of this is computers, electronics, and enterprise software. Now let me take a different text here. I'm going to copy a text which is from the Oracle News website which talks about MaxLife Insurance Company using OCI. Now if I click on analyze, We have seen language detection and the text classification outputs. Now let's go through the named entity recognition, which is capable of extracting the entities such as the organization names and the product names, geographic locations, organization names, quantities, et cetera. We can see here MaxLife Insurance Company Limited is an organization. Exadata which is a product from Oracle is correctly identified as product and MaxLife again is an organization 70-99 percent these are all identified as quantities. Keyphrase extraction helps you to identify the key terms which helps you to identify the text with those key terms. So you can use keyphrase extraction to automatically generate the tags for the given document. For example, here the text talks about MaxLife and mission critical database workloads, OCI, Exadata database service, and application performance improvement 70%. A lot of these are very good candidates for generating the tags so that you can easily identify the documents with. Let me take another text for demonstrating the sentiment analysis. So this is a review about a product a car which has been recently launched in India and I really like the car even though it's bit expensive and that is what I have captured in the review here. The Toyota Innova is very impressive with its design and luxurious interiors and smooth performance and the top notch finish etc but I have mentioned that this is bit expensive. The price is bit on the expensive. So, let s see how well the sentiment analysis. Overall, as a reviewer, I like the car, majority of the aspects I liked. That s why the document sentiment that is the overall sentiment, it is positive. The score is 71 percent. Whereas, there are aspects that are identified. For example, the design is an aspect which received 99 % positivity. Interiors, performance, exterior, cabin, and finish, all of them are the positive aspects from this product. Driving experience is also the product names like Innova, High Cross, and SU. You can also see that the price, which is another aspect of this product, has received a negative opinion as expressed by the reviewer. This is really powerful if you have customer feedbacks or survey results or product reviews that you have to analyze, not just determining whether the review is just positive or negative, but also extracting these entities is really helpful to track the performance of the product or feature perception in the market so that you can drive improvement. If you want to highlight in the dashboards, if you want to highlight which sentences are contributing to positivity or negativity, you can use sentence level sentiment as well. Let me take another text to demonstrate the PII service. So I have a text which is an email sent to a bank complaining about a fraudulent transaction. Obviously it has many private information like the name, email address and the phone number. All these are private information. and that is mentioned in the unstructured text. Let's see how the model performed. Now you can see the model has detected that John Doe is a person name and Jane Smith is the person. This is the credit card number and phone number and email address and the date and at the end again the person name is written. By default, the model masks the data by retaining last four characters, but we can change that behavior. I mentioned the number of characters to be left unmasked as four. Let me change this to zero so that every character is masked in the private information. I also have a masking character changed and let me click on analyze. Now you can see all the private information that is detected by the model is completely masked. So with this we have covered the pre-trained models, which helps you to analyze text, be it language detection, classification, named entity recognition, or personal identifiable information or sentiment analysis. Next, let us go through the text translation feature. Click on text translation. And you have on the left side you can select the source language. And the right side you can select the target language. Let me select Spanish in this case. When I click on translate, the text gets automatically translated to the target language. Now the translation can happen from any language to any language that is need not to be always to one to or fro between English and other languages but from Spanish to Arabic or Spanish to Danish or Japanese I can directly translate. Okay, so the plain text translation is very simple really helps you to translate the text like any chart conversation Or the ui labels etc. Now, let me show you how to translate html content. Let's say you have a text like this right which has H1 content h2 content and the paragraph content Now what I can do is I can copy the HTML source into OCI language console, click on Sorry, let me change the language to English. And let me translate this to French, we can translate. And we can see that the content is translated. Now let me copy the content, replace the content and let's see how the translated content looks. Now you can see here the content got translated into French, but the service has retained all the formatting of HTML. So you have seen text translation and the translation of HTML content in OCI console. Now let me show you a quick demo for custom models. Majority of the times The classification of text done by pre-trained models may not be sufficient for your use case. For example, if you are processing email complaints received in CRM systems like CBL or Service Cloud, you want to categorize the emails to the classes that you define. For example, if you are implementing the solution for an airline company, then you want to classify the content into either missing baggage, ticketing or in-flight experience and many other classes. Similarly if you have custom named entity recognition requirements taking the same example from airline industry you have a complaint and if somebody says that baggage is missed then from the email you would like to extract the origin city and the destination city probably the airport name and PNR numbers and other such You can train named entity recognition model to detect these custom entities. Let me show you how to train custom models in OCI language console. So to train a custom model, the first step is to prepare the training data set. And for custom text classification model, you have to prepare a CSV file having two columns. The first column is having the text and you should name it as text. The second column as labels. What I have done is I captured 10 samples for each of the category and I've pasted them in the in the spreadsheet here. So, this is one example for in-flight experience this is another example and similarly I have examples for missing baggage and examples for travel credits etc. Once you have the CSV file with you, you have to upload the CSV file into object storage and then you are ready to train a model. For training a model you need to create a project and I'll say demo project which will help you to organize the models into projects. Now the project creation is complete. Now you are ready to create a model for training a classification model. You have to select text classification. And since our data set is single level, I'll select single level. And the training data for classification is going to be from object storage. So let me select the place where I have uploaded the the CSV file. So, I have uploaded to the bucket with our demo model data sets and airline tickets. This is the model. This is the data I click on next specify the model name. Click on next. I review the details. This will kickstart the modern training process which will take some time. For this demo, let me take you through the model which has already been trained. Once the model training completes, the user interface shows the metrics, the precision recall and F1 score, which helps you to determine whether the model is ready for production or not. Then you can also review the class metrics. For example, I have five classes here and all of them are at 100 % F1 score which is pretty good and I have the confusion matrix which will tell you if there is any ambiguity between the classes but in this case all the classes are correctly predicted. The class matrix and the confusion matrix helps you to do the error analysis and correct the training data if required. Once you are happy with the model metrics the next step is to create a model endpoint create model endpoint specify the name here specify the number of inference units create model endpoint once the model endpoint is created you are ready to run the inference so this ticket is about cancellation and user is asking for the refund and the model correctly predicts that the request is for travel predicts and the model is able to correctly predict the category. To summarize, you need to have a CSV file as a training data set, upload that into object storage, create the model, you create an endpoint, and then you are ready for running there. For named entity recognition, you need to prepare the training data set using OCI data labeling service, which is accessible through OCI console, analytics and AI. and under data science machine learning. you have the data labeling service. Now you create the data set and click on the created data set and you annotate the entities that you want to recognize. In this case we are trying to train an NER model to process the offer letters and extract the entities like the position name, new join in name, hiring etc. For example, in this case, I have labeled Logan Giles as join name. This is the new joining and Joe's Horton as HR manager. And there are a couple of other entities which are not present in this talk. Once the labeling is complete in the data labeling service, you come to OCR language create a model by selecting the named entity recognition and the training data specify the training data as data labeling service select the data set name specify the name and click on next UI will show you the summary click on create and train the model and the model training will kick start Once the model training is complete you can review the metrics of the model the precision recall and F1 score are displayed here. And entity level metrics are also displayed. For example, here HR name, manager, company, new hire name, all those entities have 100 % F1 score. But position and salary are not doing very well. And if we examine the confusion metrics, it will show you that position was correctly predicted four times out of five. Reviewing the entity matrix and confusion matrix helps you to do the error analysis and look at the training data and correct any errors. You create the model endpoint by specifying the name, number of inference units, just like the way you did for text classification model and click on create model endpoints. For running the inference, you go to analyze, paste the text, And this text is about an offer letter given to Nathan. And the hiring manager is Shirley. Click on analyze to see the model output. Nathan Dennis is correctly identified as new hire name and Shirley is the new hire name. If you want to see the JSON output, key is the document ID and entities as an array. To summarize, language offers a managed service and offers REST APIs, SDKs, offers ready to use models and you do not need machine learning or data science expertise, offers pre-trained models and custom models for analyzing or classifying the text and translation APIs for translating the text and HTML content. Thank you for watching.